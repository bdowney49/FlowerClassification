{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23e7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # First install your OpenCV-Python if you haven't\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from scipy.cluster.vq import vq, kmeans, whiten # not used\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "from sklearn.cluster import KMeans as km # not used\n",
    "from sklearn.cluster import MiniBatchKMeans as MBkm\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer # not used\n",
    "from scipy.signal import convolve2d as conv # not used\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4a8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_model(paths):\n",
    "    sift_vector = [] # append each sift vector to the list\n",
    "    print('Performing SIFT on each image in the image categories...')\n",
    "    for cat in range(len(paths)):\n",
    "        print(f'working on category: {categories[cat]}', end='...')\n",
    "        for image_path in paths[cat]:\n",
    "            im = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) # read images one by one, converting them to gray on read\n",
    "            im = cv2.resize(im, (224, 224)) # resize image to standard 224x224 size\n",
    "            _, des = sift.detectAndCompute(im, None) # SIFT algorithm to get descriptors\n",
    "            sift_vector.append(des) # add the (keypoints, 128-d) matrix to the sift_vector list\n",
    "        print('done')\n",
    "    #print(len(sift_vector), len(sift_vector[0]), len(sift_vector[0][0])) # sift vector is images x key points x 128-d descriptors\n",
    "    sift_vector = np.asarray(sift_vector, dtype=object) # convert the list to a numpy array for use of concatenate function\n",
    "    sift_feature_matrix = np.concatenate(sift_vector, axis = 0) # concatenate along axis 0, ie the result will be (imagesxkeypoints, 128-d descriptors)\n",
    "    print('Fitting SIFT matrix to MiniBatch KMeans', end='...')\n",
    "    mbkm_model = MBkm(n_clusters=clusters, random_state=SEED).fit(sift_feature_matrix) # use cluster count, seed for reproducability and fit to the sift vector generated above!\n",
    "    print('MiniBatch Kmeans Model Produced!\\n\\n')\n",
    "    return mbkm_model, sift_feature_matrix\n",
    "\n",
    "def generate_features(paths, mbkm_model):\n",
    "    histograms_ = [] # append each histogram vector to the list\n",
    "    truth_values = [] # append category index per image\n",
    "    histogram_edges = [] # testing \n",
    "    print('Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...')\n",
    "    for cat in range(len(paths)):\n",
    "        print(f'working on category: {categories[cat]}', end='...')\n",
    "        for image_path in paths[cat]:\n",
    "            im = cv2.imread(image_path) # read image --- BGR\n",
    "            im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "            im = cv2.resize(im, (224,224)) # resize the gray images\n",
    "            #histo_edges, _ = np.histogram(cv2.pyrDown(conv(im, kern, mode='same')).flatten(), bins=int(clusters/2))\n",
    "            histo_edges, _ = np.histogram(cv2.pyrDown(cv2.Canny(im,40,100)).flatten(), bins=int(clusters/2)) # canny edge detect, downsample, flatten and run through histogram\n",
    "            mean_ = np.mean(histo_edges)\n",
    "            max_ = np.max(histo_edges)\n",
    "            min_ = np.min(histo_edges)\n",
    "            histo_edges = (histo_edges - mean_) / (max_ - min_) # mean normalization of additional features from canny edge detection\n",
    "            histogram_edges.append(histo_edges)\n",
    "            _, des = sift.detectAndCompute(im, None) # SIFT algorithm to get descriptors\n",
    "            preds = mbkm_model.predict(des) # use the model to predict the descriptor clusters\n",
    "            histogram, _ = np.histogram(preds, bins=clusters) # only need histogram values as that is the feature vector for the image\n",
    "            histograms_.append(histogram) # append the hist to the list of hists (ie feature vectors)\n",
    "            truth_values.append(cat) # build the truth values\n",
    "        print('done')\n",
    "    histogram_edges =np.asarray(histogram_edges) \n",
    "    histograms_ = np.asarray(histograms_)\n",
    "    truth_values = np.asarray(truth_values)\n",
    "    return histograms_, truth_values, histogram_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edd6def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating MiniBatch Kmeans model:\n",
      "\n",
      "Performing SIFT on each image in the image categories...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "Fitting SIFT matrix to MiniBatch KMeans...MiniBatch Kmeans Model Produced!\n",
      "\n",
      "\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n"
     ]
    }
   ],
   "source": [
    "'''kern = [[-1/9, -1/9, -1/9],\n",
    "            [-1/9, -1/9, -1/9],\n",
    "            [-1/9, -1/9, -1/9] ]''' # uncomment this is you wish to use conv layer instead of canny edge detector as extra features\n",
    "\n",
    "SEED = 42 # seed kmeans with a set value -- removed for randomness :D\n",
    "clusters = 81 # number of clusters - iteratively searched for in the cell at the bottom this notebook!\n",
    "key_points = 400 # max number of key points per image\n",
    "sift = cv2.SIFT_create(key_points) # limit the number of keypoints\n",
    "categories = ['daisy','dandelion','rose','sunflower','tulip'] # classes found inside train\n",
    "# train folders\n",
    "train_folder = './data/train/' # location of train data\n",
    "train_im_folders = [train_folder + categories[i] for i in range(len(categories))] # create list of paths for each category\n",
    "train_im_paths = [list(paths.list_images(train_im_folders[i])) for i in range(len(train_im_folders))] # get names of all images indexed by class\n",
    "# test folders\n",
    "test_folder = './data/test/' # location of test data\n",
    "test_im_folders = [test_folder + categories[i] for i in range(len(categories))] # create list of paths for each category\n",
    "test_im_paths = [list(paths.list_images(test_im_folders[i])) for i in range(len(test_im_folders))] # get names of all images indexed by class\n",
    "\n",
    "print('Generating MiniBatch Kmeans model:\\n')\n",
    "MBkmeans_model, descr = produce_model(train_im_paths)\n",
    "print('Generating Train Features:\\n')\n",
    "train_features_hists, y_train_true, train_edges  = generate_features(train_im_paths, MBkmeans_model)\n",
    "print('\\nGenerating Test Features:\\n')\n",
    "X_test_features, y_test, test_edges = generate_features(test_im_paths, MBkmeans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cce572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the feature dimensions \n",
    "train_features = np.hstack((train_features_hists, train_edges))\n",
    "X_test = np.hstack((X_test_features, test_edges))\n",
    "\n",
    "X_train, X_val, y_train, y_val = tts(train_features, y_train_true, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2be102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Random Forest\n",
      "----------------------------------------------------------\n",
      "\n",
      "Train Scores:\n",
      "Confusion Matrix: \n",
      "[[462   0   0   0   0]\n",
      " [  0 678   0   0   0]\n",
      " [  0   2 501   0   0]\n",
      " [  0   1   0 474   0]\n",
      " [  0   0   1   0 642]]\n",
      "F1-Score: 0.9985515765105095\n",
      "Accuracy: 0.9985512495472655\n",
      "\n",
      "Validation Scores:\n",
      "Confusion Matrix: \n",
      "[[ 57  30  14  11  37]\n",
      " [  6 115   7   9  26]\n",
      " [  4  14  47  11  48]\n",
      " [  6  19  12  60  14]\n",
      " [ 14  14  14  13  89]]\n",
      "F1-Score: 0.5276401645694381\n",
      "Accuracy: 0.532561505065123\n",
      "\n",
      "Test Scores:\n",
      "Confusion Matrix: \n",
      "[[ 72  28  10  12  31]\n",
      " [ 19 130  10  20  32]\n",
      " [  9  27  65  14  42]\n",
      " [ 15  22  13  78  19]\n",
      " [  4  17  28  18 130]]\n",
      "F1-Score: 0.5458432481268288\n",
      "Accuracy: 0.5491329479768786\n",
      "\n",
      "----------------------------------------------------------\n",
      "Multinomial Logistic Regression\n",
      "----------------------------------------------------------\n",
      "\n",
      "Train Scores:\n",
      "Confusion Matrix: \n",
      "[[202 124  42  37  57]\n",
      " [ 34 530  24  30  60]\n",
      " [ 23  93 248  55  84]\n",
      " [ 29  63  36 300  47]\n",
      " [ 37 116  77  56 357]]\n",
      "F1-Score: 0.586892882746897\n",
      "Accuracy: 0.5929011227816009\n",
      "\n",
      "Validation Scores:\n",
      "Confusion Matrix: \n",
      "[[ 64  37  12  21  15]\n",
      " [ 12 119   7  11  14]\n",
      " [ 10  30  33  16  35]\n",
      " [  9  12   6  75   9]\n",
      " [  8  26  20  11  79]]\n",
      "F1-Score: 0.5228137970519103\n",
      "Accuracy: 0.5354558610709117\n",
      "\n",
      "Test Scores:\n",
      "Confusion Matrix: \n",
      "[[ 61  46  12  17  17]\n",
      " [ 13 155  10  22  11]\n",
      " [  6  40  70  15  26]\n",
      " [ 10  17  15  84  21]\n",
      " [  7  36  25  22 107]]\n",
      "F1-Score: 0.5455744983072371\n",
      "Accuracy: 0.5514450867052023\n",
      "\n",
      "----------------------------------------------------------\n",
      "SVM\n",
      "----------------------------------------------------------\n",
      "\n",
      "Train Scores:\n",
      "Confusion Matrix: \n",
      "[[283  67  27  14  71]\n",
      " [ 25 550   9  24  70]\n",
      " [ 17  55 296  29 106]\n",
      " [ 27  32  24 347  45]\n",
      " [ 24  65  62  38 454]]\n",
      "F1-Score: 0.697832609102806\n",
      "Accuracy: 0.6990220934444042\n",
      "\n",
      "Validation Scores:\n",
      "Confusion Matrix: \n",
      "[[ 68  29  13  14  25]\n",
      " [ 11 115   5  13  19]\n",
      " [  6  18  42  16  42]\n",
      " [  8  10   7  70  16]\n",
      " [ 11  27  15  17  74]]\n",
      "F1-Score: 0.5286582231094467\n",
      "Accuracy: 0.5340086830680174\n",
      "\n",
      "Test Scores:\n",
      "Confusion Matrix: \n",
      "[[ 72  34   6  15  26]\n",
      " [ 20 140   9  19  23]\n",
      " [  6  31  67  14  39]\n",
      " [ 14  11  13  84  25]\n",
      " [  4  44  35  21  93]]\n",
      "F1-Score: 0.5247223152576237\n",
      "Accuracy: 0.5271676300578034\n",
      "\n",
      "----------------------------------------------------------\n",
      "Ensemble Results of test set predictions from Random Forest, Ridge Regression and SVM\n",
      "----------------------------------------------------------\n",
      "Confusion Matrix: \n",
      "[[ 74  37   6  16  20]\n",
      " [ 14 155   7  18  17]\n",
      " [  9  35  69  12  32]\n",
      " [ 17  15  12  85  18]\n",
      " [  7  31  24  20 115]]\n",
      "\n",
      "F1-Score: 0.5710871399501084\n",
      "Accuracy: 0.5757225433526012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data for training and testing!\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.linear_model import RidgeClassifier as LR_ridge\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "print('----------------------------------------------------------')\n",
    "print('Random Forest')\n",
    "print('----------------------------------------------------------')\n",
    "RF_model = RF(random_state=SEED, max_depth=12, n_estimators=1000).fit(X_train, y_train)\n",
    "print('\\nTrain Scores:')\n",
    "pred_train = RF_model.predict(X_train)\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_train, pred_train)}')\n",
    "print(f'F1-Score: {f1_score(y_train, pred_train, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_train, pred_train)}')\n",
    "print('\\nValidation Scores:')\n",
    "pred_val = RF_model.predict(X_val)\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_val, pred_val)}')\n",
    "print(f'F1-Score: {f1_score(y_val, pred_val, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_val, pred_val)}')\n",
    "print('\\nTest Scores:')\n",
    "pred_test_rf = RF_model.predict(X_test)\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, pred_test_rf)}')\n",
    "print(f'F1-Score: {f1_score(y_test, pred_test_rf, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, pred_test_rf)}\\n')\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "print('Multinomial Logistic Regression')\n",
    "print('----------------------------------------------------------')\n",
    "print('\\nTrain Scores:')\n",
    "LR_model = LR_ridge(max_iter=800, alpha=10e-3).fit(X_train, y_train)\n",
    "pred_train = LR_model.predict(X_train)\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_train, pred_train)}')\n",
    "print(f'F1-Score: {f1_score(y_train, pred_train, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_train, pred_train)}')\n",
    "print('\\nValidation Scores:')\n",
    "pred_val = LR_model.predict(X_val)\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_val, pred_val)}')\n",
    "print(f'F1-Score: {f1_score(y_val, pred_val, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_val, pred_val)}')\n",
    "print('\\nTest Scores:')\n",
    "pred_test_lr = LR_model.predict(X_test)\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, pred_test_lr)}')\n",
    "print(f'F1-Score: {f1_score(y_test, pred_test_lr, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, pred_test_lr)}\\n')\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "print('SVM')\n",
    "print('----------------------------------------------------------')\n",
    "print('\\nTrain Scores:')\n",
    "SVM_model = SVC().fit(X_train, y_train)\n",
    "pred_train = SVM_model.predict(X_train)\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_train, pred_train)}')\n",
    "print(f'F1-Score: {f1_score(y_train, pred_train, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_train, pred_train)}')\n",
    "print('\\nValidation Scores:')\n",
    "pred_val = SVM_model.predict(X_val)\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_val, pred_val)}')\n",
    "print(f'F1-Score: {f1_score(y_val, pred_val, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_val, pred_val)}')\n",
    "print('\\nTest Scores:')\n",
    "pred_test_svm = SVM_model.predict(X_test)\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, pred_test_svm)}')\n",
    "print(f'F1-Score: {f1_score(y_test, pred_test_svm, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, pred_test_svm)}\\n')\n",
    "\n",
    "pred_test_ensemble = []\n",
    "for i in range(pred_test_rf.shape[0]):\n",
    "    mode = stats.mode([pred_test_rf[i], pred_test_lr[i], pred_test_svm[i],])[0][0]\n",
    "    pred_test_ensemble.append(mode)\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "print('Ensemble Results of test set predictions from Random Forest, Ridge Regression and SVM')\n",
    "print('----------------------------------------------------------')\n",
    "print(f'Confusion Matrix: \\n{confusion_matrix(y_test, pred_test_ensemble)}')\n",
    "print(f'\\nF1-Score: {f1_score(y_test, pred_test_ensemble, average=\"weighted\")}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, pred_test_ensemble)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f48d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Shown above are my train, validation and test results across three different ML classifiers. \n",
    "I used Random Forests, Ridge Regression and SVM. To achieve slighly higher results I used a type of ensemble\n",
    "of these classifiers, by taking the mode of each prediction (if there is no mode than the smallest index is taken)\n",
    "and generating a new prediction. I achieved an F1 Score of ~0.571 and Accuracy of ~0.575 with the ensemble\n",
    "method!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98408245",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster: 65\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "RF f1: 0.5206279252397833\n",
      "LR f1: 0.521714951522194\n",
      "SVM f1: 0.5179768535281327\n",
      "\n",
      "cluster: 67\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "RF f1: 0.5038056934720823\n",
      "LR f1: 0.5038034539634253\n",
      "SVM f1: 0.5114643854876547\n",
      "\n",
      "cluster: 69\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "RF f1: 0.525633700928601\n",
      "LR f1: 0.517704060027356\n",
      "SVM f1: 0.5101697992191743\n",
      "\n",
      "cluster: 71\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "RF f1: 0.5262212311107863\n",
      "LR f1: 0.5279835166219649\n",
      "SVM f1: 0.534115253594944\n",
      "\n",
      "cluster: 73\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "RF f1: 0.5371083404086261\n",
      "LR f1: 0.52981353085428\n",
      "SVM f1: 0.5323330991874311\n",
      "\n",
      "cluster: 75\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "RF f1: 0.5248411834785016\n",
      "LR f1: 0.5533311672454847\n",
      "SVM f1: 0.5383811775363034\n",
      "\n",
      "cluster: 77\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "RF f1: 0.5114452113793905\n",
      "LR f1: 0.5294292977643233\n",
      "SVM f1: 0.5139370632554192\n",
      "\n",
      "cluster: 79\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "RF f1: 0.5226356660990271\n",
      "LR f1: 0.5302867597978974\n",
      "SVM f1: 0.5546875850104607\n",
      "\n",
      "cluster: 81\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "\n",
      "Generating Test Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy...done\n",
      "working on category: dandelion...done\n",
      "working on category: rose...done\n",
      "working on category: sunflower...done\n",
      "working on category: tulip...done\n",
      "RF f1: 0.5462754241060397\n",
      "LR f1: 0.5514606566218654\n",
      "SVM f1: 0.5145373032145186\n",
      "\n",
      "cluster: 83\n",
      "Fitting SIFT matrix to MiniBatch KMeans\n",
      "Generating Train Features:\n",
      "\n",
      "Generating feature vectors for each image using sift descriptors, kmeans model predict and histogram...\n",
      "working on category: daisy..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2700/215853357.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclustie\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclusters_range\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\ncluster: {clustie}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mrf_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterative_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclustie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'RF f1: {rf_acc}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'LR f1: {lr_acc}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2700/215853357.py\u001b[0m in \u001b[0;36miterative_testing\u001b[1;34m(clusties)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmbkm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMBkm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclusties\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Generating Train Features:\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtrain_features_hists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_edges\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mgenerate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_im_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmbkm_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nGenerating Test Features:\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mX_test_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_edges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_im_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmbkm_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2700/3384259031.py\u001b[0m in \u001b[0;36mgenerate_features\u001b[1;34m(paths, mbkm_model)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mhisto_edges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhisto_edges\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# mean normalization of additional features from canny edge detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mhistogram_edges\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhisto_edges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# SIFT algorithm to get descriptors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmbkm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# use the model to predict the descriptor clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mhistogram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# only need histogram values as that is the feature vector for the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cluster Count Hyper-Parameter Iterative Testing\n",
    "# Long Run Time Warning!\n",
    "\n",
    "# variable 'descr' is from above code, it must be run first. \n",
    "# =====================================================================\n",
    "\n",
    "def iterative_testing(clusties):\n",
    "    print('Fitting SIFT matrix to MiniBatch KMeans')\n",
    "    mbkm_model = MBkm(n_clusters=clusties, random_state=SEED).fit(descr)\n",
    "    print('Generating Train Features:\\n')\n",
    "    train_features_hists, y_train_true, train_edges  = generate_features(train_im_paths, mbkm_model)\n",
    "    print('\\nGenerating Test Features:\\n')\n",
    "    X_test_features, y_test, test_edges = generate_features(test_im_paths, mbkm_model)\n",
    "    \n",
    "    train_features = np.hstack((train_features_hists, train_edges))\n",
    "    X_test = np.hstack((X_test_features, test_edges))\n",
    "    X_train, X_val, y_train, y_val = tts(train_features, y_train_true, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    RF_model = RF(random_state=SEED, max_depth=12, n_estimators=1000).fit(X_train, y_train)\n",
    "    pred_test = RF_model.predict(X_test)\n",
    "    RF_accuracy = f1_score(y_test, pred_test, average=\"weighted\")\n",
    "    \n",
    "    LR_model = LR_ridge(max_iter=800, alpha=10e-3).fit(X_train, y_train)\n",
    "    pred_test = LR_model.predict(X_test)\n",
    "    LR_accuracy = f1_score(y_test, pred_test, average=\"weighted\")\n",
    "    \n",
    "    SVM_model = SVC().fit(X_train, y_train)\n",
    "    pred_test = SVM_model.predict(X_test)\n",
    "    SVM_accuracy = f1_score(y_test, pred_test, average=\"weighted\")\n",
    "    \n",
    "    return RF_accuracy, LR_accuracy, SVM_accuracy\n",
    "\n",
    "clusters_range = np.arange(65,95, 2)\n",
    "RF_acc_by_cluster = []\n",
    "LR_acc_by_cluster = []\n",
    "SVM_acc_by_cluster = []\n",
    "\n",
    "for clustie in clusters_range:\n",
    "    print(f'\\ncluster: {clustie}')\n",
    "    rf_acc, lr_acc, svm_acc = iterative_testing(clustie)\n",
    "    print(f'RF f1: {rf_acc}')\n",
    "    print(f'LR f1: {lr_acc}')\n",
    "    print(f'SVM f1: {svm_acc}')\n",
    "    RF_acc_by_cluster.append(rf_acc)\n",
    "    LR_acc_by_cluster.append(lr_acc)\n",
    "    SVM_acc_by_cluster.append(svm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = len(clusters_range)\n",
    "plt.plot(x_range, RF_acc_by_cluster)\n",
    "plt.plot(x_range, LR_acc_by_cluster)\n",
    "plt.plot(x_range, SVM_acc_by_cluster)\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('cluster count')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
